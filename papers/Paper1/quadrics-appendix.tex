\appendix
% \appendixpage
% \addappheadtotoc
\section{Parabola of Revolution}
\label{app:parabola}

In this section we develop the particular case of the parabola.
The parametrization is given by:

\begin{align}
x &= - \frac{1}{2}R_ct^2 + R_0 \\
y &= R_c t
\end{align}
Where $R_c$ is the radius of curvature and $R_0$ is the distance of the parabola nose to the
origin.
The respective tridimensional shape is given by:
\begin{align}
x &= -\frac{1}{2}R_ct^2 + R_0 \label{eq:x-par-a}\\
y &= R_c t \cos\phi  \label{eq:y-par-a}\\
z &= R_c t \sin\phi  \label{eq:z-par-a}
\end{align}
The azimutal angle where the surface is tangent to the line of sight in this case is given by:
\begin{align}
\sin\phi_t = -\frac{\tan i}{t} \label{eq:sin-tan-a} 
\end{align}

Subtituting (\ref{eq:sin-tan-a}) into (\ref{eq:x-par-a}), (\ref{eq:y-par-a}) and
(\ref{eq:z-par-a}) we find the apparent shape
of the paraboloid:

\begin{align}
x' &= -\frac{R_c(\frac{1}{2}t^2 \cos^2 i -\sin^2 i)}{\cos i}+R_0\cos i \\
y' &= R_c\left(t^2-\tan^2 i\right)^{1/2} 
\end{align}

Taking the limit of equations (\ref{eq:qprime}) and (\ref{eq:Aprime}) when $\theta_c$ tends to zero we find that:

\begin{align}
\left(\frac{q'}{q}\right)_{\mathrm{parabola}} &= 1+\frac{\tilde{R}_c\tan^2 i}{2}\\
\left(\tilde{R}'_c\right)_{\mathrm{parabola}} &= \frac{\tilde{R}_c}{\cos^2 i + \frac{\tilde{R}_c}{2}\sin^2 i}
\end{align}

\section{Analytic derivation of the radius of curvature in the Thin Shell model}
\label{app:rc-analytic}

For small $\theta$ we may do a polynomial expansion for the shell shape such as:

\begin{align}
R \simeq R_0\left(1+\gamma\theta^2 + \Gamma\theta^4\right) \label{eq:R-exp}
\end{align}

The radius of curvature at the axis for $R$ is given by:

\begin{align}
R_c = R_0\left(1-2\gamma\right)^{-1}
\end{align}

The coefficient gamma may be derived by an expansion at small angles of equation
(\ref{eq:th1th}), as follows:

From the first term of the right side we get:

\begin{align}
\cot\theta &\simeq \theta^{-1}\left[1-\frac{1}{3}\theta^2\right] \\
\cos^k\theta\sin^2\theta &\simeq \theta^2 - \left(\frac{1}{3} + \frac{k}{2}\right)\theta^4 \\
\implies I_k(\theta) &\simeq \frac{1}{3}\theta^3\left[ 1 - \frac{1}{10}(3k+2)\theta^2\right]\\
\implies 2\beta I_k(\theta)\cot\theta &\simeq \frac{2}{3}\beta\theta^2\left[1-\frac{1}{30}
(9k+16)\theta^2\right]\label{eq:AR1} 
\end{align}

For the second term we get:

\begin{align}
-\frac{2\beta}{k+2}\left(1-\cos^{k+2}\theta\right) & \simeq -\beta\theta^2\left[1-\frac{1}{12}
(3k+4)\theta^2\right] \label{eq:AR2}
\end{align}

For the left side we use equation (25) from \CRW{}. Then, equation (\ref{eq:th1th}) results
as follows:

\begin{align}
\theta_1^2\left(1+\frac{1}{15}\theta_1^2\right) = \beta\theta^2\left[1+\frac{1}{60}(4-9k)
\theta^2\right] \label{eq:th1th-app}
\end{align}

And we can use the approximation $\theta_1 \approx \beta\theta^2$ for the correction term in
the left side of (\ref{eq:th1th-app}):

\begin{align}
\theta_1^2 &= \beta\theta^2\left[1+\frac{1}{60}(4-9k)\theta^2\right]
\left(1-\frac{\beta}{15}\theta^2\right) \\
\implies \theta_1^2 &= \beta\theta^2\left[1+ 2C_{k\beta}\theta^2\right]
\label{eq:th1th-small}\\
\mathrm{where:~} C_{k\beta} &\equiv \frac{1}{2}\left(A_k-\frac{\beta}{15}\right) \\
A_k &\equiv \frac{1}{15}-\frac{3k}{20}
\end{align}

Now, using equation (23) from \CRW{} we may estimate $R$ at low angles. To do this, we need to
expand each term as follows (neglecting terms of order four or higher):

\begin{align}
\theta_1 = &= \beta^{1/2}\theta\left[1+ 2C_{k\beta}\theta^2\right]^{1/2} \\
\theta + \theta_1 &= \theta\left[1+\beta^{1/2}\left(1+2C_{k\beta}\theta^2\right)\right]\\
\sin\theta_1 &= \theta_1\left[1-\frac{\theta_1^2}{6}\right] \\
 &= \beta^{1/2}\theta\left[1+\left(C_{k\beta}-\frac{1}{6}\beta\right)\theta^2\right] \\
 \sin(\theta+\theta_1) &= \left[\theta+\theta_1\right]\left[1-\frac{\left(\theta+\theta_1
 \right)^2}{6}\right] \\
 &= \theta\left(1+\beta^{1/2}\right)\left\lbrace 1+\left[\frac{C_{k\beta}\beta^{1/2}}
 {1+\beta^{1/2}}-\frac{1}{6}\left(1+\beta^{1/2}\right)^2\right]\theta^2\right\rbrace
\end{align}


So, combining these terms with equation (23) from \CRW{} we found the final expression for $R$:

\begin{align}
\frac{R}{D}\equiv \frac{\sin\theta_1}{\sin(\theta+\theta_1)} = \frac{\beta^1/2}{1+\beta^{1/2}}
\left\lbrace 1 + \theta^2\left[\frac{C_{k\beta}}{1+\beta^{1/2}}+\frac{1}{6}\left(1+2\beta^{1/2}
\right)\right] \right\rbrace \label{eq:r-small-theta}
\end{align}

Returning to equation (\ref{eq:R-exp}) we see the following:

\begin{align}
R_0 &= \frac{\beta^{1/2}}{1+\beta^{1/2}} \\
\gamma &= \frac{C_{k\beta}}{1+\beta^{1/2}}+\frac{1}{6}\left(1+2\beta^{1/2}\right)
\label{eq:app-gamma}
\end{align}

We recover equation (27) of \CRW{} for $R_0$ and equation (\ref{eq:app-gamma}) is the
needed term to calculate the radius of curvature at the axis.

\section{Analytic derivation of \texorpdfstring{\boldmath $R_{90}$}{R\_90} in the thin shell model}
\label{app:r90-analytic}

To derive $R_{90}$ we need to evaluate equations (23) from \CRW{} and (\ref{eq:th1th})
at $\theta=\frac{\pi}{2}$:

\begin{align}
R_{90} = D\tan\theta_{1,90} \\
\theta_{1,90}\cot\theta_{1,90} -1 = -\frac{2\beta}{k+2} \label{eq:th190}
\end{align}
Where $\theta_{1,90}\equiv \theta_1(\frac{\pi}{2})$. Combining both equations and  introducing
the parameter $\xi\equiv \frac{2}{k+2}$ we have:
\begin{align}
R_{90} &= D\frac{\theta_{1,90}}{1-\xi\beta} \label{eq:r90-incomplete}
\end{align}

Expanding the left side of (\ref{eq:th190}) until fourth order, equation (\ref{eq:th190})
becomes:

\begin{align}
\theta_{1,90}^2\left(1+\frac{\theta_{1,90}^2}{15}\right) \simeq 3\xi\beta
\end{align}

Applying the approximation $\theta_1^2 \approx 3\xi\beta$ we found a solution
for $\theta_{1,90}$:

\begin{align}
\theta_{1,90} = \left(\frac{3\xi\beta}{1+\frac{1}{5}\xi\beta}\right)^{1/2}
\end{align}

And substituting into (\ref{eq:r90-incomplete}) we find the solution for $R_{90}$:

\begin{align}
R_{90} &= \frac{\left(3\xi\beta\right)^{1/2}}{\left(1+\frac{1}{5}\xi\beta\right)^{1/2}
\left(1-\xi\beta\right)} \\
\implies \tilde{R}_{90} &\equiv \frac{R_{90}}{R_0} = \frac{\sqrt{3\xi}\left(1+\beta^{1/2}\right)}
{\left(1+\frac{1}{5}\xi\beta\right)^{1/2}\left(1-\xi\beta\right)}
\end{align}

\section{Derivation of Characteristic Radii in Isotropic Wind/Parallel interaction Problem}
\label{app:ch-rad-Wilkin}

$\tilde{R}_{90}$ is obtained by simply evaluating equation (\ref{eq:R-Wilkin}) at $\theta=\frac{\pi}{2}$.
For the Radius of curvature we follow a similar procedure than the Wind/Wind interaction, but using equation
(\ref{eq:R-Wilkin}) for $R(\theta)$ and inserting the cosecant into the square root.

Expanding the terms of $R(\theta)$ we find the following:

\begin{align}
  \csc^2\theta &\simeq \theta^{-2}\left[1+\frac{\theta^2}{3}\left(1+\frac{\theta^2}{5}\right)\right] \\
  1-\theta\cot\theta &\simeq \frac{\theta^2}{3}\left[1 + \frac{\theta^2}{15}\left(1+\frac{2\theta^2}{21}\right)\right] \\
  \implies \tilde{R}(\theta) \simeq 1 + \frac{\theta^2}{5} + O\left[\theta^4\right]
\end{align}

From equation (\ref{eq:Rcurv}) for the radius of curvature we finally get the numerical value for $\tilde{R_c} = \frac{5}{3}$
\clearpage
\sisetup{detect-weight=true, detect-shape=true, detect-inline-weight=math}
\sisetup{round-mode=figures, round-precision=2}
\begin{landscape}
\begin{table}
  \setlength\tabcolsep{2pt}
  \caption{Big table of \(p\)-values}
  \begin{tabular}{@{} ll @{\ } S[round-mode=places]S[round-mode=places] S[round-mode=places]S[round-mode=places] SS SS @{\quad\quad\quad} SSS @{\quad} S@{}S@{}S @{}}\toprule
     & & \multicolumn{2}{c}{Mean} & \multicolumn{2}{c}{Std.\ Dev.} & \multicolumn{2}{c}{Obs.\ Disp.} & \multicolumn{2}{c @{\quad\quad\quad}}{s.e.m.} & \multicolumn{3}{c @{\quad} }{\dotfill Effect sizes\dotfill } & \multicolumn{3}{c}{\dotfill Non-parametric test \(p\)-values  \dotfill} \\ 
    {Comparison} & {Variable} & {\(\langle \text{A} \rangle\)} & {\(\langle \text{B} \rangle\)} & {\(\sigma_{\text{A}}\)} & {\(\sigma_{\text{B}}\)} & {\(\langle \epsilon_{\text{A}} \rangle\)} & {\(\langle \epsilon_{\text{B}} \rangle\)} & {\((\sigma/\!\sqrt n)_{\text{A}}\)} & {\((\sigma/\!\sqrt n)_{\text{B}}\)} & {\(r_b\)} & {\(d_C\)} & {\(\sigma_{\text{A}}/\sigma_{\text{B}}\)} & \multicolumn{1}{c}{Anderson--Darling} & \multicolumn{1}{c}{Rank biserial} &  \multicolumn{1}{c}{Brown--Forsythe}\\
\midrule
    \multicolumn{10}{@{} l @{\quad\quad\quad}}{\itshape Median split of continuous variables \dotfill}\\
    \addlinespace
    Faint/bright & \(R_{90}\) & 1.677 & 1.768 & 0.269 & 0.316 & 0.231 & 0.236 & 0.025 & 0.030 & \bfseries 0.174 & \bfseries -0.308 & 0.851 & \bfseries 0.0215 & \bfseries 0.0235 & 0.125\\
\(H\) magnitude  & \(\Delta R_{90}\) & 0.183 & 0.198 & 0.161 & 0.163 &  &  & 0.015 & 0.015 & 0.070 & -0.093 & 0.987 & 0.538 & 0.365 & 0.761\\
\(n_{\text{A}} =  n_{\text{B}} = 113\) & \(R_{c}\) & 1.655 & 1.917 & 0.631 & 1.045 & 0.097 & 0.078 & 0.059 & 0.098 & 0.123 & -0.303 & \bfseries 0.604 & 0.123 & 0.111 & \bfseries 0.0335\\
\addlinespace
Low/high  & \(R_{90}\) & 1.707 & 1.739 & 0.250 & 0.336 & 0.256 & 0.212 & 0.024 & 0.031 & 0.061 & -0.110 & \bfseries 0.745 & \bfseries 0.0281 & 0.428 & \bfseries 0.00139\\
\(R_0\) & \(\Delta R_{90}\) & 0.175 & 0.204 & 0.157 & 0.165 &  &  & 0.015 & 0.015 & 0.091 & -0.176 & 0.948 & 0.103 & 0.238 & 0.193\\
\(n_{\text{A}} =  n_{\text{B}} = 113\) & \(R_{c}\) & 1.766 & 1.803 & 0.975 & 0.755 & 0.114 & 0.062 & 0.092 & 0.071 & 0.100 & -0.043 & 1.291 & 0.228 & 0.192 & 0.599\\
\addlinespace
Low/high & \(R_{90}\) & 1.703 & 1.742 & 0.267 & 0.323 & 0.233 & 0.235 & 0.025 & 0.030 & 0.040 & -0.132 & 0.824 & 0.554 & 0.602 & 0.123\\
extinction & \(\Delta R_{90}\) & 0.186 & 0.195 & 0.138 & 0.183 &  &  & 0.013 & 0.017 & -0.039 & -0.057 & 0.754 & 0.301 & 0.61 & 0.112\\
\(n_{\text{A}} =  n_{\text{B}} = 113\)  & \(R_{c}\) & 1.725 & 1.846 & 0.822 & 0.917 & 0.091 & 0.085 & 0.077 & 0.086 & 0.082 & -0.139 & 0.896 & 0.219 & 0.285 & 0.982\\
\addlinespace
Low/high & \(R_{90}\) & 1.722 & 1.724 & 0.328 & 0.261 & 0.234 & 0.234 & 0.031 & 0.024 & 0.020 & -0.008 & 1.256 & 0.308 & 0.795 & 0.0534\\
\(\vert{}b\vert\)  & \(\Delta R_{90}\) & 0.188 & 0.191 & 0.161 & 0.162 &  &  & 0.015 & 0.015 & 0.009 & -0.021 & 0.995 & 0.964 & 0.907 & 0.694\\
\(n_{\text{A}} =  n_{\text{B}} = 113\) & \(R_{c}\) & 1.706 & 1.862 & 0.727 & 0.988 & 0.085 & 0.091 & 0.068 & 0.092 & 0.069 & -0.181 & 0.736 & 0.19 & 0.368 & 0.0842\\
\addlinespace
High/low & \(R_{90}\) & 1.734 & 1.707 & 0.279 & 0.321 & 0.241 & 0.223 & 0.024 & 0.034 & -0.049 & 0.093 & 0.868 & 0.361 & 0.532 & 0.159\\
\(\cos \ell\)  & \(\Delta R_{90}\) & 0.182 & 0.201 & 0.155 & 0.171 &  &  & 0.013 & 0.018 & 0.054 & -0.122 & 0.909 & 0.604 & 0.491 & 0.365\\
\(n_{\text{A}}, n_{\text{B}} = 137, 90\) & \(R_{c}\) & 1.807 & 1.751 & 0.946 & 0.742 & 0.090 & 0.084 & 0.081 & 0.078 & -0.000 & 0.064 & 1.274 & 1.03 & 0.999 & 0.549\\
\midrule
    \multicolumn{10}{@{} l @{\quad\quad\quad}}{\itshape Categorical variables \dotfill}\\
    \addlinespace
Environment:  & \(R_{90}\) & 1.735 & 1.693 & 0.283 & 0.338 & 0.238 & 0.218 & 0.022 & 0.053 & -0.070 & 0.142 & 0.838 & 0.603 & 0.49 & 0.392\\
Facing & \(\Delta R_{90}\) & 0.190 & 0.195 & 0.161 & 0.172 &  &  & 0.012 & 0.027 & -0.019 & -0.034 & 0.938 & 0.507 & 0.85 & 0.438\\
\(n_{\text{A}}, n_{\text{B}} = 170, 41\) & \(R_{c}\) & 1.757 & 1.852 & 0.854 & 0.899 & 0.087 & 0.083 & 0.066 & 0.140 & 0.042 & -0.110 & 0.950 & 0.713 & 0.676 & 0.377\\
\addlinespace
Environment:  & \(R_{90}\) & 1.735 & 1.680 & 0.283 & 0.309 & 0.238 & 0.233 & 0.022 & 0.077 & -0.130 & 0.193 & 0.916 & 0.518 & 0.391 & 0.782\\
\hii & \(\Delta R_{90}\) & 0.190 & 0.175 & 0.161 & 0.138 &  &  & 0.012 & 0.034 & -0.048 & 0.095 & 1.170 & 0.932 & 0.754 & 0.799\\
\(n_{\text{A}}, n_{\text{B}} = 170, 16\) & \(R_{c}\) & 1.757 & 1.907 & 0.854 & 0.955 & 0.087 & 0.105 & 0.066 & 0.239 & 0.024 & -0.174 & 0.894 & 0.496 & 0.875 & 0.255\\
\addlinespace
Single/multiple & \(R_{90}\) & 1.709 & 1.762 & 0.289 & 0.315 & 0.230 & 0.243 & 0.022 & 0.041 & 0.074 & -0.177 & 0.917 & 0.342 & 0.396 & 0.338\\
source candidate & \(\Delta R_{90}\) & 0.184 & 0.206 & 0.162 & 0.160 &  &  & 0.013 & 0.021 & 0.093 & -0.136 & 1.013 & 0.421 & 0.284 & 0.97\\
\(n_{\text{A}}, n_{\text{B}} = 167, 60\) & \(R_{c}\) & 1.767 & 1.833 & 0.825 & 0.988 & 0.090 & 0.080 & 0.064 & 0.128 & 0.027 & -0.076 & 0.835 & 0.999 & 0.756 & 0.605\\
\addlinespace
With/without  & \(R_{90}\) & 1.734 & 1.721 & 0.289 & 0.298 & 0.223 & 0.237 & 0.043 & 0.022 & -0.042 & 0.044 & 0.970 & 0.595 & 0.667 & 0.563\\
\SI{8}{\um} emission & \(\Delta R_{90}\) & 0.203 & 0.186 & 0.205 & 0.149 &  &  & 0.031 & 0.011 & 0.021 & 0.106 & 1.377 & 0.765 & 0.826 & 0.219\\
\(n_{\text{A}}, n_{\text{B}} = 45, 182\) & \(R_{c}\) & 1.714 & 1.802 & 0.598 & 0.926 & 0.091 & 0.087 & 0.089 & 0.069 & -0.012 & -0.101 & 0.646 & 0.824 & 0.904 & 0.2\\
\midrule
    \multicolumn{10}{@{} l @{\quad\quad\quad}}{\itshape Intercomparison with other datasets \dotfill}\\
    \addlinespace
MIPS vs Orion & \(R_{90}\) & 1.723 & 2.418 & 0.297 & 0.811 &  &  & 0.020 & 0.191 & \bfseries 0.700 & \bfseries -1.928 & \bfseries 0.366 & \bfseries 8.02e-06 & \bfseries 7.84e-07 & \bfseries 6.41e-06\\
 & \(\Delta R_{90}\) & 0.190 & 0.506 & 0.162 & 0.551 &  &  & 0.011 & 0.130 & 0.220 & -1.468 & \bfseries 0.294 & \bfseries 3.6e-05 & 0.121 & \bfseries 4.03e-14\\
\(n_{\text{A}}, n_{\text{B}} = 227, 18\) & \(R_{c}\) & 1.784 & 2.639 & 0.871 & 1.302 &  &  & 0.058 & 0.307 & \bfseries 0.516 & \bfseries -0.940 & 0.669 & \bfseries 0.000505 & \bfseries 0.000273 & 0.12\\
\addlinespace
MIPS vs RSG & \(R_{90}\) & 1.723 & 1.386 & 0.297 & 0.122 &  &  & 0.020 & 0.046 & \bfseries -0.756 & \bfseries 1.152 & 2.425 & \bfseries 0.000494 & \bfseries 0.000671 & 0.0573\\
 & \(\Delta R_{90}\) & 0.190 & 0.197 & 0.162 & 0.296 &  &  & 0.011 & 0.112 & -0.242 & -0.045 & 0.546 & 0.156 & 0.276 & 0.397\\
\(n_{\text{A}}, n_{\text{B}} = 227, 7\) & \(R_{c}\) & 1.784 & 1.426 & 0.871 & 0.079 &  &  & 0.058 & 0.030 & -0.201 & 0.418 & 10.969 & 0.0742 & 0.367 & 0.0505\\
\bottomrule
\end{tabular}
\end{table}
\end{landscape}

\section{Distribution of p-values for all correlations tested}
\label{sec:distr-p-values}

\begin{figure}
  (a)\\
  \includegraphics[width=\linewidth]{figs/p-value-histogram-new-linear}\\
  (b)\\
  \includegraphics[width=\linewidth]{figs/p-value-histogram-new}
  \caption{Histogram of \(p\)-values for all non-parametric 2-sample
    tests performed during the analysis of
    section~\ref{sec:comp-with-observ}. (a)~Uniformly spaced linear
    bins and linear vertical axis. (b)~Uniformly spaced logarithmic
    bins and logarithmic vertical axis, with all values
    \(p \le 10^{-6}\) included in the leftmost bin.  The vertical dashed
    lines shows the traditional threshold values for significance:
    \(p = 0.001\) and \(p = 0.05\). The red solid line shows the
    uniform distribution of \(p\)-values that would be expected if the
    null hypothesis were always true, that is, if no significant
    correlations existed.}
  \label{fig:histo-p-values}
\end{figure}



\begin{table}
  \caption{Lower bounds on the Type~I error rate, \(\alpha\)}
  \label{tab:type-I}
  \centering
  \begin{tabular}{lrr} \toprule
    Correlation & \(p\) & \(\alpha\) \\
    \midrule
    \multicolumn{3}{l}{\itshape Quantitative \dotfill}\\
    \(R_c\) vs \(R_0\) & \textit{0.0054} & \textit{0.0712} \\
    \(R_{90}\) vs \(R_0\) & \textbf{0.0001} & \textbf{0.0025} \\
    \(R_c\) vs \(H_0\) & 0.1229 & 0.4119 \\
    \(R_{90}\) vs \(H_0\) & \textit{0.0215} & \textit{0.1833} \\
    \(R_c\) vs \(A_K\) & 0.19 & 0.4617 \\
    \(R_{90}\) vs \(A_K\) & 0.63 & 1 \\
    \(R_c\) vs \(|b|\) & 0.19 & 0.4617 \\
    \(R_{90}\) vs \(|b|\) & 0.31 & 0.4967 \\
    \(R_c\) vs \(\cos(\ell)\) & 1.00 & 1 \\
    \(R_{90}\) vs \(\cos(\ell)\) & 0.36 & 0.4999 \\
    \midrule
    \multicolumn{3}{l}{\itshape Categorical} \dotfill\\
    \(R_c\) vs Facing & 0.71 & 1 \\
    \(R_c\) vs H II & 0.50 & 1 \\
    \(R_{90}\) vs Facing & 0.60 & 1 \\
    \(R_{90}\) vs H II & 0.52 & 1 \\
    \(R_c\) vs Multiple & 1.00 & 1 \\
    \(R_{90}\) vs Multiple & 0.34 & 0.4993 \\
    \(R_c\) vs \SI{8}{\um} & 0.82 & 1 \\
    \(R_{90}\) vs \SI{8}{\um} & 0.6 & 1 \\
    \midrule
    \multicolumn{3}{l}{\itshape Other datasets} \dotfill\\
    \(R_c\) vs Herschel & 0.074 & 0.3437 \\
    \(R_{90}\) vs Herschel & \textbf{0.00052} & \textbf{0.0106} \\
    \(R_c\) vs M42 & \textbf{0.00048} & \textbf{0.0099} \\
    \(R_{90}\) vs M42 & \textbf{0.0000105} & \textbf{0.0003} \\
    \bottomrule
  \end{tabular}
\end{table}


The \(p\)-values are the probability of finding a difference between
two populations as large as (or larger than) what is observed,
\emph{given} that there is no difference in the underlying
distribution from which the two populations are drawn (that is, given
that the null hypothesis is true).  However, what we really want to
know is something else: the probability that the null hypothesis is
true, \emph{given} the observations.  That is, the probability,
\(\alpha\), of a \textit{false positive}, also known as the \textit{Type I
  error rate}.  The common mistake of conflating these two definitions
is known as the ``\(p\)-value fallacy'' \citep{Goodman:1999a}, or``the
error of the transposed conditional'', as discussed in detail by
\citet{Colquhoun:2014a}.  It is possible to derive \(\alpha\) from
\(p\) using Bayes' theorem (e.g., \citealp{Goodman:1999b}), but that
requires an estimate of the prior probability of the null hypothesis,
independent of the observations.  Alternatively, it is also possible
to find a lower bound on \(\alpha\) from a frequentist approach
\citep{Sellke:2001a}:
\begin{equation}
  \label{eq:type-I}
  \alpha(p) \ge \bigg[ 1 - \big(e\, p \ln p\big)^{-1} \bigg]^{-1}
  \quad \text{valid for } p < 1/e.
\end{equation}
This is the approach we adopt here, which also numerically coincides
with the Bayesian approach for the case where the prior probability of
the null hypothesis is 0.5.  The reason that this is only a lower
limit for \(\alpha\) is that if we have overwhelming a priori evidence that
the null hypothesis is true (for instance, from previous empirical
studies, or because it follows from a well-supported theory), then a
Bayesian calculation would give a much higher value of \(\alpha\) than
\eqref{eq:type-I} does.  In our case, however, we have no strong
reasons for favoring any of the null hypotheses, so it is reasonable
to assume \(\alpha\) is close to the lower limit given in \eqref{eq:type-I}.

In order to choose a threshold \(p\)-value that counts as a
``significant'' result, one then needs to balance the risks of false
positives against the risks of \textit{false negatives}.  The false
negative probability, \(\beta\), also known as \textit{Type II error
  rate}, is the probability of failing to reject an untrue null
hypothesis.  That is, in the context of this paper, it is the
probability of failing to detect a real difference between two
sub-samples, or a real correlation between two variables.  The
complementary probability, \(1 - \beta\), is known as the
\textit{statistical power} or sensitivity of the test.  The value of
\(\beta\) depends on three factors:
\begin{enumerate}[1.]
\item The \textit{effect size}, which is a measure of the magnitude of
  the difference in a dependent variable between two sub-samples, or
  the degree of correlation between two continuous variables.  For the
  two sub-sample case, it is common to use a standardised mean
  difference, such as Cohen's \(d\) statistic \citep{Cohen:1988a}:
  \(d = (\bar{X}_A - \bar{X}_B) / s\), where \(\bar{X}_A\),
  \(\bar{X}_B\) are the means of the dependent variable \(X\) for
  samples A and B, while \(s\) is the pooled standard deviation of
  \(X\).  For the case of two continuous variables, the Pearson linear
  correlation coefficient, \(r\), can be used.  In both cases, rules
  of thumb have been developed \citep{Ruscio:2008a} for classifying an
  effect as ``large'' (\(d > 0.8\), \(r > 0.4\)) or ``small''
  (\(d < 0.2\), \(r < 0.1\)).  Alternatively, non-parametric
  statistics can be used, such as the \(A\) measure of stochastic
  superiority \citep{Delaney:2002a}.
\end{enumerate}

Obviously, this depends on the \textit{effect size}, which is the 


All astronomical data analysis is \emph{post hoc} analysis, since the universe was not set up to test a particular hypothesis (as far as we know).  It is therefore important to guard against the ``multiple comparisons problem'', whereby seemingly significant correlations are found where none really exist, simply by virtue of the large number of tests that were carried out.

Under the more conservative Holm--Bonferroni method, only comparisons with \(p < 0.001\) would be significant. 

The p-curve \citep{Head:2015a}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "quadrics-bowshock.tex"
%%% End:
